# data_collector.py
import os
import pandas as pd
import yfinance as yf
import time
import logging
import json

# è¨­å®š logging æ ¼å¼
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

# ğŸ“‚ è¨­å®šå¿«å–ç›®éŒ„
CACHE_DIR = "cache"
AI_DATABASE = "ai_brain.json"  # AI è¨˜æ†¶åº«
os.makedirs(CACHE_DIR, exist_ok=True)  # ç¢ºä¿å¿«å–è³‡æ–™å¤¾å­˜åœ¨

def load_ai_memory():
    """ åŠ è¼‰ AI çš„è¨˜æ†¶åº« """
    if os.path.exists(AI_DATABASE):
        with open(AI_DATABASE, "r", encoding="utf-8") as file:
            return json.load(file)
    return {}

def save_ai_memory(memory):
    """ å„²å­˜ AI çš„è¨˜æ†¶åº« """
    with open(AI_DATABASE, "w", encoding="utf-8") as file:
        json.dump(memory, file, indent=4, ensure_ascii=False)

def analyze_prediction_error(stock_symbol, actual_price, predicted_price, error_threshold=5):
    """ è‹¥é æ¸¬èª¤å·®è¶…éé–¾å€¼ï¼Œå‰‡åˆ†æèª¤å·®åŸå› ä¸¦æ›´æ–° AI è¨˜æ†¶åº« """
    error = abs(actual_price - predicted_price)
    if error > error_threshold:
        logging.info(f"ğŸ” {stock_symbol} é æ¸¬èª¤å·® {error} è¶…é {error_threshold}ï¼Œé€²è¡Œ AI åˆ†æ")
        ai_memory = load_ai_memory()
        reason = f"AI åˆ†æç™¼ç¾ {stock_symbol} é æ¸¬èª¤å·®è¶…é {error_threshold}ï¼Œéœ€è¦æ”¹é€²ã€‚"  # æ¨¡æ“¬ AI æ€è€ƒéç¨‹
        ai_memory[stock_symbol] = {
            "last_actual_price": actual_price,
            "last_predicted_price": predicted_price,
            "error": error,
            "reason": reason
        }
        save_ai_memory(ai_memory)
        logging.info(f"ğŸ“š AI è¨˜æ†¶åº«å·²æ›´æ–° {stock_symbol}")

def get_stock_data(stock_symbol, period="10y", interval="1d", max_retries=3, force_download=False):
    """
    ä¸‹è¼‰è‚¡ç¥¨æ•¸æ“šï¼Œä¸¦å„²å­˜åˆ°æœ¬åœ°å¿«å–ï¼Œé¿å…é‡è¤‡ä¸‹è¼‰ã€‚ç¢ºä¿ 'Close' æ¬„ä½æ ¼å¼æ­£ç¢ºï¼Œè§£æ±ºæ™‚å€éŒ¯èª¤ã€‚
    """
    try:
        cache_file = os.path.join(CACHE_DIR, f"{stock_symbol}_{interval}.csv")

        # **ğŸ“‚ å˜—è©¦è®€å–æœ¬åœ°å¿«å–**
        if os.path.exists(cache_file) and not force_download:
            logging.info(f"ğŸ“‚ ç™¼ç¾ {stock_symbol} çš„æœ¬åœ°å¿«å–ï¼Œå˜—è©¦è®€å–...")
            df_last = pd.read_csv(cache_file, index_col="Date", parse_dates=True).tail(1)

            # **ç¢ºä¿ index æ˜¯ tz-naive**
            last_cached_date = df_last.index[-1] if not df_last.empty else None
            if last_cached_date is not None and last_cached_date.tz is not None:
                last_cached_date = last_cached_date.tz_localize(None)

            # **å–å¾—æœ€æ–°çš„äº¤æ˜“æ—¥**
            try:
                market_today = yf.download(stock_symbol, period="1d", interval="1d").index[-1].tz_localize(None)
            except Exception as e:
                logging.warning(f"âš ï¸ ç„¡æ³•ç²å– {stock_symbol} æœ€æ–°äº¤æ˜“æ—¥ï¼Œä½¿ç”¨ `pd.Timestamp.today()` ä»£æ›¿")
                market_today = pd.Timestamp.today().normalize().tz_localize(None)

            # **æª¢æŸ¥å¿«å–æ˜¯å¦æœ€æ–°**
            if last_cached_date and last_cached_date >= market_today:
                logging.info(f"âœ… {stock_symbol} çš„æ•¸æ“šå·²æ˜¯æœ€æ–°ï¼")
                df = pd.read_csv(cache_file, index_col="Date", parse_dates=True)  # è®€å–å®Œæ•´æ•¸æ“š
                return df

            logging.info(f"ğŸ”„ {stock_symbol} çš„æ•¸æ“šéèˆŠï¼Œé‡æ–°ä¸‹è¼‰...")

        # **ğŸ“¥ ä¸‹è¼‰æ–°æ•¸æ“š**
        for attempt in range(1, max_retries + 1):
            try:
                logging.info(f"ğŸ“¥ å˜—è©¦ä¸‹è¼‰ {stock_symbol} çš„æ•¸æ“š (ç¬¬ {attempt} æ¬¡)...")
                stock = yf.Ticker(stock_symbol)
                df = stock.history(period=period, interval=interval)

                if df.empty:
                    raise ValueError(f"âš ï¸ {stock_symbol} æ²’æœ‰å¯ç”¨æ•¸æ“šï¼")

                # **è™•ç† MultiIndex æ¬„ä½**
                if isinstance(df.columns, pd.MultiIndex):
                    df.columns = df.columns.droplevel(0)  # ç§»é™¤ç¬¬ä¸€å±¤ç´¢å¼•

                # **ç¢ºä¿ 'Close' æ¬„ä½å­˜åœ¨**
                if "Close" not in df.columns:
                    raise ValueError(f"âŒ {stock_symbol} ç¼ºå°‘ 'Close' æ¬„ä½ï¼Œè«‹æª¢æŸ¥ä¸‹è¼‰çš„æ•¸æ“š")

                # **ç¢ºä¿ 'Close' æ˜¯æ•¸å€¼å‹æ…‹ä¸¦è½‰æ›ç‚º Series**
                df["Close"] = pd.to_numeric(df["Close"], errors="coerce")
                df.dropna(subset=["Close"], inplace=True)

                # **ä¿®æ­£æ™‚å€å•é¡Œ**
                if isinstance(df.index, pd.DatetimeIndex):
                    df.index = df.index.tz_localize(None)

                # **å­˜å…¥æœ¬åœ°å¿«å–**
                df.to_csv(cache_file)
                logging.info(f"ğŸ’¾ {stock_symbol} çš„æ•¸æ“šå·²å„²å­˜åˆ°æœ¬åœ°å¿«å– ({cache_file})")

                return df

            except Exception as e:
                logging.warning(f"âš ï¸ {stock_symbol} ä¸‹è¼‰å¤±æ•—: {str(e)}")
                if attempt < max_retries:
                    logging.info(f"ğŸ”„ ç­‰å¾… 5 ç§’å¾Œé‡è©¦...")
                    time.sleep(5)  # ç­‰å¾… 5 ç§’å¾Œé‡è©¦
                else:
                    logging.error(f"âŒ {stock_symbol} æ•¸æ“šä¸‹è¼‰å®Œå…¨å¤±æ•—ï¼Œè«‹ç¨å¾Œå†è©¦ï¼")
                    return None  # æ”¾æ£„è©²è‚¡ç¥¨çš„æ•¸æ“š

    except Exception as e:
        logging.error(f"âŒ ç²å– {stock_symbol} æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None
